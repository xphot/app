{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "438cd9a7102f44a8babc806425a7cafd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4214ab1ba8484bfd97418d7df31ccc68",
              "IPY_MODEL_9d859429867d4801bbf8ed919754f0c3",
              "IPY_MODEL_5e839cfce915499bbb774a08ef0c97f6"
            ],
            "layout": "IPY_MODEL_a34792e22b0446ef9e11a0f155ae7bbb"
          }
        },
        "4214ab1ba8484bfd97418d7df31ccc68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cad5f58f35e747e2b627748f99f584c0",
            "placeholder": "​",
            "style": "IPY_MODEL_374ce797a4324789ac32fd3713fb05bd",
            "value": "Depth=0, working on node 10: 100%"
          }
        },
        "9d859429867d4801bbf8ed919754f0c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e59a0a8e0b548f6993389239f2652ef",
            "max": 11,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_faccd24cb802402ea7bf7f6607e48f0d",
            "value": 11
          }
        },
        "5e839cfce915499bbb774a08ef0c97f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fde3181f4ee4f808be9e47173441b0a",
            "placeholder": "​",
            "style": "IPY_MODEL_e0817167f2df430f87a14984ec6e32f4",
            "value": " 11/11 [00:00&lt;00:00, 357.07it/s]"
          }
        },
        "a34792e22b0446ef9e11a0f155ae7bbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cad5f58f35e747e2b627748f99f584c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "374ce797a4324789ac32fd3713fb05bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e59a0a8e0b548f6993389239f2652ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faccd24cb802402ea7bf7f6607e48f0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9fde3181f4ee4f808be9e47173441b0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0817167f2df430f87a14984ec6e32f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "438cd9a7102f44a8babc806425a7cafd",
            "4214ab1ba8484bfd97418d7df31ccc68",
            "9d859429867d4801bbf8ed919754f0c3",
            "5e839cfce915499bbb774a08ef0c97f6",
            "a34792e22b0446ef9e11a0f155ae7bbb",
            "cad5f58f35e747e2b627748f99f584c0",
            "374ce797a4324789ac32fd3713fb05bd",
            "2e59a0a8e0b548f6993389239f2652ef",
            "faccd24cb802402ea7bf7f6607e48f0d",
            "9fde3181f4ee4f808be9e47173441b0a",
            "e0817167f2df430f87a14984ec6e32f4"
          ]
        },
        "id": "IpFsRSri2jrh",
        "outputId": "3aff53b7-339f-436f-952c-db519c2e0de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.2.15)\n",
            "Requirement already satisfied: causal-learn in /usr/local/lib/python3.11/dist-packages (0.1.4.0)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.46.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
            "Requirement already satisfied: unsloth_zoo>=2025.2.7 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2025.2.7)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.6.0)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.0.29.post3)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.9.16)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.3.2)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.26.4)\n",
            "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.15.2)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.14.0)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.28.1)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.32.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.21.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from causal-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from causal-learn) (1.6.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from causal-learn) (0.20.3)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from causal-learn) (0.14.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from causal-learn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from causal-learn) (3.10.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from causal-learn) (3.4.2)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.11/dist-packages (from causal-learn) (3.0.4)\n",
            "Requirement already satisfied: momentchi2 in /usr/local/lib/python3.11/dist-packages (from causal-learn) (0.1.8)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from shap) (0.61.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.11.13)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (13.9.4)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.2.7->unsloth) (25.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.2.7->unsloth) (11.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers->unsloth) (4.6.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->shap) (0.44.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->causal-learn) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->causal-learn) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->causal-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->causal-learn) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->causal-learn) (1.0.1)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (1.7.1)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->causal-learn) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (2.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (0.1.2)\n",
            "Warning: Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs! LLM functionalities will be disabled.\n",
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/11 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "438cd9a7102f44a8babc806425a7cafd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KDE plot of errors and task_difficulty created.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-4d8d1713760e>:307: UserWarning: \n",
            "The palette list has fewer values (2) than needed (3) and will cycle, which may produce an uninterpretable plot.\n",
            "  sns.violinplot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Violin plot: errors by group created.\n",
            "Plot export failed: \n",
            "Image export using the \"kaleido\" engine requires the kaleido package,\n",
            "which can be installed using pip:\n",
            "    $ pip install -U kaleido\n",
            "\n",
            "Radar plot: errors vs participant_id created.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-4d8d1713760e>:409: UserWarning:\n",
            "\n",
            "This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bipartite graph of participants and features created.\n",
            "Insights saved to: /content/drive/MyDrive/output_errors_mistral_grok/insights.txt\n",
            "DDQN Agent Training Completed\n",
            "Execution completed successfully.\n",
            "\n",
            "--- Fine-tuning section skipped due to missing GPU or LLM setup ---\n",
            "\n",
            "--- GGUF Quantization section skipped due to missing GPU or LLM setup ---\n"
          ]
        }
      ],
      "source": [
        "!pip install unsloth causal-learn shap transformers plotly accelerate bitsandbytes\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "import shap\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import plotly.express as px\n",
        "from scipy.stats import bootstrap\n",
        "import numpy as np\n",
        "from causallearn.search.ConstraintBased import FCI\n",
        "from causallearn.utils.GraphUtils import GraphUtils\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from io import StringIO\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "import torch\n",
        "\n",
        "# --- GPU Check and Unsloth Import ---\n",
        "try:\n",
        "    from unsloth import FastLanguageModel\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"Warning: NVIDIA GPU not found. Unsloth and LLM functionalities will be disabled.\")\n",
        "        USE_UNSLOTH_LLM = False\n",
        "    else:\n",
        "        print(\"NVIDIA GPU found. Unsloth and LLM functionalities will be enabled.\")\n",
        "        USE_UNSLOTH_LLM = True\n",
        "except ImportError:\n",
        "    print(\"Warning: unsloth library not found. LLM functionalities will be disabled.\")\n",
        "    USE_UNSLOTH_LLM = False\n",
        "except NotImplementedError as e:\n",
        "    print(f\"Warning: {e} LLM functionalities will be disabled.\")\n",
        "    USE_UNSLOTH_LLM = False\n",
        "except Exception as e:\n",
        "    print(f\"Warning: An unexpected error occurred during unsloth import: {e}. LLM functionalities will be disabled.\")\n",
        "    USE_UNSLOTH_LLM = False\n",
        "\n",
        "\n",
        "# --- Google Drive Configuration ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# Output Path in Google Drive\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/output_errors_mistral_grok/\"\n",
        "\n",
        "# DataFrame Column Names\n",
        "PARTICIPANT_ID_COLUMN: str = \"participant_id\"\n",
        "GROUP_COLUMN: str = \"group\"\n",
        "ERRORS_COLUMN: str = \"errors\"\n",
        "TASK_DIFFICULTY_COLUMN: str = \"task_difficulty\"\n",
        "ERROR_TYPE_1_COLUMN: str = \"error_type_1\"\n",
        "ERROR_TYPE_2_COLUMN: str = \"error_type_2\"\n",
        "ERROR_TYPE_3_COLUMN: str = \"error_type_3\"\n",
        "TASK_FEATURE_1_COLUMN: str = \"task_feature_1\"\n",
        "TASK_FEATURE_2_COLUMN: str = \"task_feature_2\"\n",
        "TASK_FEATURE_3_COLUMN: str = \"task_feature_3\"\n",
        "\n",
        "# LLM Models\n",
        "# We'll use Unsloth for efficient fine-tuning\n",
        "MODEL_MISTRAL_NAME: str = \"unsloth/mistral-7b-v0.3-bnb-4bit\"  # Example Unsloth model\n",
        "MODEL_GROK_NAME: str = \"unsloth/mistral-7b-v0.3-bnb-4bit\" # Using the same for simplicity, can be different\n",
        "MODEL_GROK_ENHANCED_NAME: str = \"unsloth/mistral-7b-v0.3-bnb-4bit\" # Using the same for simplicity, can be different\n",
        "\n",
        "LINE_WIDTH: float = 2.5\n",
        "BOOTSTRAP_RESAMPLES: int = 500\n",
        "\n",
        "# --- Synthetic Dataset ---\n",
        "SYNTHETIC_DATASET: str = \"\"\"\n",
        "participant_id,group,errors,task_difficulty,error_type_1,error_type_2,error_type_3,task_feature_1,task_feature_2,task_feature_3\n",
        "P001,Group 1,5,7,2,1,0,0.8,0.5,3\n",
        "P002,Group 1,3,6,1,0,2,0.6,0.3,2\n",
        "P003,Group 1,6,8,3,2,1,0.9,0.7,4\n",
        "P004,Group 2,8,9,4,3,0,0.7,0.9,5\n",
        "P005,Group 2,4,7,0,2,1,0.5,0.4,1\n",
        "P006,Group 2,7,8,3,1,2,0.8,0.6,3\n",
        "P007,Group 1,2,5,1,0,0,0.4,0.2,2\n",
        "P008,Group 2,9,9,4,4,3,0.9,0.8,4\n",
        "P009,Group 1,4,6,2,1,1,0.7,0.5,3\n",
        "P010,Group 2,6,7,3,0,2,0.6,0.7,2\n",
        "P011,Group 3,7,8,2,3,1,0.8,0.9,4\n",
        "P012,Group 3,5,6,1,2,0,0.7,0.6,3\n",
        "P013,Group 3,3,5,0,1,2,0.5,0.4,1\n",
        "P014,Group 3,8,9,4,3,3,0.9,0.8,5\n",
        "P015,Group 1,6,7,2,2,1,0.8,0.7,3\n",
        "P016,Group 2,4,6,1,0,2,0.6,0.5,2\n",
        "P017,Group 1,9,9,4,4,0,0.9,0.9,4\n",
        "P018,Group 2,2,5,0,1,1,0.4,0.3,1\n",
        "P019,Group 3,7,8,3,2,2,0.8,0.7,3\n",
        "P020,Group 1,5,7,2,1,0,0.7,0.6,2\n",
        "\"\"\"\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def create_output_directory(path: str) -> None:\n",
        "    \"\"\"Creates the output directory if it doesn't exist.\"\"\"\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def load_data_from_string(csv_string: str) -> pd.DataFrame:\n",
        "    \"\"\"Loads data from a CSV string.\"\"\"\n",
        "    csv_file = StringIO(csv_string)\n",
        "    return pd.read_csv(csv_file)\n",
        "\n",
        "def validate_dataframe(df: pd.DataFrame, required_columns: List[str]) -> bool:\n",
        "    \"\"\"Validates the DataFrame: checks for missing columns, data types,\n",
        "    duplicate IDs, and valid group names.\n",
        "    \"\"\"\n",
        "    if df is None:\n",
        "        print(\"Error: DataFrame is None. Cannot validate.\")\n",
        "        return False\n",
        "\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "    if missing_columns:\n",
        "        print(f\"Error: Missing columns: {missing_columns}\")\n",
        "        return False\n",
        "\n",
        "    for col in required_columns:\n",
        "        if col not in (PARTICIPANT_ID_COLUMN, GROUP_COLUMN):\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                print(f\"Error: Non-numeric data in column: {col}\")\n",
        "                return False\n",
        "\n",
        "    if df[PARTICIPANT_ID_COLUMN].duplicated().any():\n",
        "        print(\"Error: Duplicate participant IDs found.\")\n",
        "        return False\n",
        "\n",
        "    valid_groups = [\"Group 1\", \"Group 2\", \"Group 3\"]\n",
        "    invalid_groups = df[~df[GROUP_COLUMN].isin(valid_groups)][GROUP_COLUMN].unique()\n",
        "    if invalid_groups.size > 0:\n",
        "        print(f\"Error: Invalid group names found: {invalid_groups}\")\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def analyze_text_with_llm(text: str, model_name: str, model, tokenizer) -> str:\n",
        "    \"\"\"Analyzes the given text using the specified LLM (loaded model).\"\"\"\n",
        "    try:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\") # Move inputs to GPU\n",
        "        outputs = model.generate(**inputs, max_new_tokens=150, do_sample=True)\n",
        "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error with LLM {model_name}: {e}\")\n",
        "        return f\"Analysis unavailable ({model_name}).\"\n",
        "\n",
        "# --- DDQN Implementation (Improved) ---\n",
        "class DDQNAgent:\n",
        "    def __init__(self, state_dims: List[int], action_dim: int):\n",
        "        self.state_dims = state_dims\n",
        "        self.state_dim = sum(state_dims)  # Total dimension of the flattened state\n",
        "        self.action_dim = action_dim\n",
        "        # Initialize Q-network and Target network with random values\n",
        "        self.q_network = np.random.rand(self.state_dim, action_dim)\n",
        "        self.target_network = np.random.rand(self.state_dim, action_dim)\n",
        "        self.update_target_network() # Ensure target network is initialized\n",
        "\n",
        "    def act(self, state: np.ndarray, epsilon: float = 0.1) -> int:\n",
        "        \"\"\"Chooses an action using an epsilon-greedy policy.\"\"\"\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.choice(self.action_dim)  # Explore\n",
        "        else:\n",
        "            return np.argmax(self.q_network.T @ state)  # Exploit: Use dot product\n",
        "\n",
        "    def learn(self, batch: List[Tuple[np.ndarray, int, float, np.ndarray]], gamma: float = 0.99, learning_rate: float = 0.01) -> None:\n",
        "        \"\"\"Updates the Q-network using a batch of experiences.\"\"\"\n",
        "        for state, action, reward, next_state in batch:\n",
        "            # Calculate target Q-value\n",
        "            q_next = self.target_network.T @ next_state\n",
        "            q_target = reward + gamma * np.max(q_next)\n",
        "\n",
        "            # Calculate predicted Q-value\n",
        "            q_predict = self.q_network.T[action] @ state\n",
        "\n",
        "            # Update Q-network\n",
        "            self.q_network.T[action] += learning_rate * (q_target - q_predict) * state\n",
        "\n",
        "\n",
        "    def update_target_network(self) -> None:\n",
        "        \"\"\"Copies the weights from the Q-network to the target network.\"\"\"\n",
        "        self.target_network = np.copy(self.q_network)\n",
        "\n",
        "# --- Data Preprocessing ---\n",
        "\n",
        "def scale_data(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"Scales the specified columns using MinMaxScaler.\"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    df[columns] = scaler.fit_transform(df[columns])\n",
        "    return df\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Preprocesses the data by performing one-hot encoding on the 'group' column\n",
        "    and scaling the numerical columns. Returns both the transformed DataFrame\n",
        "    and a copy of the original DataFrame.\n",
        "    \"\"\"\n",
        "    df_original = df.copy()  # Store a copy of the original DataFrame\n",
        "    df = pd.get_dummies(df, columns=[GROUP_COLUMN], prefix=GROUP_COLUMN)\n",
        "    encoded_group_cols = [col for col in df.columns if col.startswith(f\"{GROUP_COLUMN}_\")]\n",
        "    df = scale_data(\n",
        "        df,\n",
        "        [\n",
        "            ERRORS_COLUMN,\n",
        "            TASK_DIFFICULTY_COLUMN,\n",
        "            ERROR_TYPE_1_COLUMN,\n",
        "            ERROR_TYPE_2_COLUMN,\n",
        "            ERROR_TYPE_3_COLUMN,\n",
        "            TASK_FEATURE_1_COLUMN,\n",
        "            TASK_FEATURE_2_COLUMN,\n",
        "            TASK_FEATURE_3_COLUMN,\n",
        "        ]\n",
        "        + encoded_group_cols,\n",
        "    )\n",
        "    return df, df_original\n",
        "\n",
        "# --- Causal Structure Discovery ---\n",
        "\n",
        "def discover_causal_structure(df: pd.DataFrame, variables: List[str], output_path: str) -> Optional[str]:\n",
        "    \"\"\"Discovers the causal structure using FCI and saves the graph.\"\"\"\n",
        "    try:\n",
        "        df_encoded = df.copy()\n",
        "        for col in df_encoded.select_dtypes(include=\"object\").columns:\n",
        "            le = LabelEncoder()\n",
        "            df_encoded[col] = le.fit_transform(df_encoded[col])\n",
        "        data_fci = df_encoded[variables].to_numpy()\n",
        "        cg = FCI.fci(data_fci)\n",
        "\n",
        "        # Check if a graph was actually found\n",
        "        if cg is None or cg[0] is None or cg[0].graph is None:\n",
        "            print(\"Warning: FCI did not find a causal graph.\")\n",
        "            return None\n",
        "\n",
        "        pdy = GraphUtils.to_pydot(cg[0])  # Access the graph correctly\n",
        "        pdy.write_png(os.path.join(output_path, \"causal_graph.png\"))\n",
        "        edges_info = [\n",
        "            f\"Node {variables[i]}->Node {variables[j]}: Type {edge_type}\"\n",
        "            for i, adj in enumerate(cg[0].graph)\n",
        "            for j, edge_type in enumerate(adj)\n",
        "            if edge_type != 0\n",
        "        ]\n",
        "        return \"\\n\".join(edges_info)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in causal discovery: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Feature Importance (SHAP) ---\n",
        "\n",
        "def calculate_shap_values(df: pd.DataFrame, feature_columns: List[str], target_column: str, output_path: str) -> Optional[str]:\n",
        "    \"\"\"Calculates and visualizes SHAP values.\"\"\"\n",
        "    try:\n",
        "        df_encoded = df.copy()\n",
        "        for col in feature_columns:\n",
        "            if df_encoded[col].dtype == \"object\":\n",
        "                le = LabelEncoder()\n",
        "                df_encoded[col] = le.fit_transform(df_encoded[col])\n",
        "\n",
        "        model_rf = RandomForestRegressor(random_state=42).fit(\n",
        "            df_encoded[feature_columns], df_encoded[target_column]\n",
        "        )\n",
        "        explainer = shap.TreeExplainer(model_rf)\n",
        "        shap_values = explainer.shap_values(df_encoded[feature_columns])\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.style.use(\"dark_background\")\n",
        "        shap.summary_plot(\n",
        "            shap_values, df_encoded[feature_columns], show=False, color_bar=True\n",
        "        )\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{output_path}shap_summary.png\")\n",
        "        plt.close()\n",
        "        return f\"SHAP for {feature_columns} predicting {target_column}\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating SHAP values: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Visualization ---\n",
        "\n",
        "def create_kde_plot(df: pd.DataFrame, column1: str, column2: str, output_path: str, colors: List[str]) -> None:\n",
        "    \"\"\"Creates a KDE plot of two columns.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.style.use(\"dark_background\")\n",
        "    sns.kdeplot(\n",
        "        data=df[column1],\n",
        "        color=colors[0],\n",
        "        label=column1.capitalize(),\n",
        "        linewidth=LINE_WIDTH,\n",
        "    )\n",
        "    sns.kdeplot(\n",
        "        data=df[column2],\n",
        "        color=colors[1],\n",
        "        label=column2.capitalize(),\n",
        "        linewidth=LINE_WIDTH,\n",
        "    )\n",
        "    plt.title(\"KDE Plot\", fontsize=16, color=\"white\")\n",
        "    plt.legend(facecolor=\"black\", edgecolor=\"white\", labelcolor=\"white\")\n",
        "    plt.grid(alpha=0.2, linestyle=\"--\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_path}kde_plot.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def create_violin_plot(df: pd.DataFrame, x_column: str, y_column: str, output_path: str, colors: List[str]) -> None:\n",
        "    \"\"\"Creates a violin plot.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.style.use(\"dark_background\")\n",
        "    sns.violinplot(\n",
        "        data=df,\n",
        "        x=x_column,\n",
        "        y=y_column,\n",
        "        palette=colors[:2],\n",
        "        linewidth=LINE_WIDTH,\n",
        "        hue=x_column,\n",
        "        legend=False,\n",
        "    )\n",
        "    plt.title(\"Violin Plot\", fontsize=16, color=\"white\")\n",
        "    plt.xlabel(x_column.capitalize(), fontsize=14, color=\"white\")\n",
        "    plt.ylabel(y_column.capitalize(), fontsize=14, color=\"white\")\n",
        "    plt.grid(alpha=0.2, linestyle=\"--\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_path}violin_plot.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def create_radar_plot(df: pd.DataFrame, r_column: str, theta_column: str, output_path: str, colors: List[str]) -> None:\n",
        "    \"\"\"Creates a radar plot.\"\"\"\n",
        "    df_copy = df.copy()\n",
        "    df_copy[theta_column] = df_copy[theta_column].astype(str)\n",
        "    fig = px.line_polar(\n",
        "        df_copy,\n",
        "        r=r_column,\n",
        "        theta=theta_column,\n",
        "        line_close=True,\n",
        "        color_discrete_sequence=colors,\n",
        "        template=\"plotly_dark\",\n",
        "    )\n",
        "    fig.update_traces(line=dict(width=LINE_WIDTH))\n",
        "    fig.update_layout(title=\"Radar Plot\")\n",
        "    try:\n",
        "        fig.write_image(f\"{output_path}radar_plot.png\")\n",
        "    except Exception as e:\n",
        "        print(f\"Plot export failed: {e}\")\n",
        "\n",
        "def create_visualizations(df: pd.DataFrame, df_original: pd.DataFrame, output_path: str, colors: List[str]) -> None:\n",
        "    \"\"\"Creates KDE, violin, and radar plots.\"\"\"\n",
        "    create_kde_plot(df, ERRORS_COLUMN, TASK_DIFFICULTY_COLUMN, output_path, colors[:2])\n",
        "    print(f\"KDE plot of {ERRORS_COLUMN} and {TASK_DIFFICULTY_COLUMN} created.\")\n",
        "    create_violin_plot(df_original, GROUP_COLUMN, ERRORS_COLUMN, output_path, colors)  # Use original df for group\n",
        "    print(f\"Violin plot: {ERRORS_COLUMN} by {GROUP_COLUMN} created.\")\n",
        "    create_radar_plot(df_original, ERRORS_COLUMN, PARTICIPANT_ID_COLUMN, output_path, colors[:3])  # Use original df for errors\n",
        "    print(f\"Radar plot: {ERRORS_COLUMN} vs {PARTICIPANT_ID_COLUMN} created.\")\n",
        "\n",
        "\n",
        "# --- Hypergraph and Bipartite Graph Conversion ---\n",
        "\n",
        "def create_feature_nodes(df: pd.DataFrame, shap_values: np.ndarray, feature_names: List[str], threshold_factor: float = 0.5) -> Dict[str, List[str]]:\n",
        "    \"\"\"Creates feature nodes for the bipartite graph based on SHAP values.\"\"\"\n",
        "    feature_nodes: Dict[str, List[str]] = {}\n",
        "    mean_shap_values = np.abs(shap_values).mean(axis=0)\n",
        "    threshold = threshold_factor * np.mean(mean_shap_values)\n",
        "    for i, feature_name in enumerate(feature_names):\n",
        "        if mean_shap_values[i] > threshold:\n",
        "            if df[feature_name].dtype in [np.float64, np.int64]:\n",
        "                median_val = df[feature_name].median()\n",
        "                feature_nodes[f\"{feature_name}_High\"] = (\n",
        "                    df[df[feature_name] >= median_val][PARTICIPANT_ID_COLUMN].tolist()\n",
        "                )\n",
        "                feature_nodes[f\"{feature_name}_Low\"] = (\n",
        "                    df[df[feature_name] < median_val][PARTICIPANT_ID_COLUMN].tolist()\n",
        "                )\n",
        "            else:\n",
        "                for unique_val in df[feature_name].unique():\n",
        "                    feature_nodes[f\"{feature_name}_{unique_val}\"] = (\n",
        "                        df[df[feature_name] == unique_val][\n",
        "                            PARTICIPANT_ID_COLUMN\n",
        "                        ].tolist()\n",
        "                    )\n",
        "    return feature_nodes\n",
        "\n",
        "def visualize_bipartite_graph(df: pd.DataFrame, feature_nodes: Dict[str, List[str]], output_path: str, colors: List[str]) -> None:\n",
        "    \"\"\"Visualizes the bipartite graph.\"\"\"\n",
        "    G = nx.Graph()\n",
        "    participant_ids = df[PARTICIPANT_ID_COLUMN].tolist()\n",
        "    G.add_nodes_from(participant_ids, bipartite=0)\n",
        "    feature_node_names = list(feature_nodes.keys())\n",
        "    G.add_nodes_from(feature_node_names, bipartite=1)\n",
        "    for feature, participants in feature_nodes.items():\n",
        "        for participant in participants:\n",
        "            G.add_edge(participant, feature)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.style.use(\"dark_background\")\n",
        "    pos = nx.bipartite_layout(G, participant_ids)\n",
        "    node_colors = [\n",
        "        colors[0] if node in participant_ids else colors[1] for node in G.nodes()\n",
        "    ]\n",
        "    nx.draw(\n",
        "        G,\n",
        "        pos,\n",
        "        with_labels=True,\n",
        "        node_color=node_colors,\n",
        "        font_color=\"white\",\n",
        "        edge_color=\"gray\",\n",
        "        width=LINE_WIDTH / 2,\n",
        "        node_size=700,\n",
        "        font_size=10,\n",
        "        alpha=0.9,\n",
        "    )\n",
        "    plt.title(\"Bipartite Graph\", fontsize=16, color=\"white\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_path}bipartite_graph.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# --- Statistical Analysis ---\n",
        "\n",
        "def perform_bootstrap(data: pd.Series, statistic: callable, n_resamples: int = BOOTSTRAP_RESAMPLES) -> Tuple[float, float]:\n",
        "    \"\"\"Performs bootstrap resampling and returns the confidence interval.\"\"\"\n",
        "    result = bootstrap(\n",
        "        (data,), statistic, n_resamples=n_resamples, method=\"percentile\", random_state=42\n",
        "    )\n",
        "    return result.confidence_interval\n",
        "\n",
        "\n",
        "def save_summary(df: pd.DataFrame, bootstrap_ci: Tuple[float, float], output_path: str) -> str:\n",
        "    \"\"\"Calculates and saves summary statistics.\"\"\"\n",
        "    summary_text = df.describe().to_string() + f\"\\nBootstrap CI: {bootstrap_ci}\"\n",
        "    with open(f\"{output_path}summary.txt\", \"w\") as f:\n",
        "        f.write(summary_text)\n",
        "    return summary_text\n",
        "\n",
        "def perform_statistical_analysis(df: pd.DataFrame, output_path: str) -> str:\n",
        "    \"\"\"Performs statistical analysis and saves the summary.\"\"\"\n",
        "    bootstrap_ci = perform_bootstrap(df[ERRORS_COLUMN], np.mean)\n",
        "    summary_stats_text = save_summary(df, bootstrap_ci, output_path)\n",
        "    return summary_stats_text\n",
        "\n",
        "def generate_insights_report(summary_text: str, causal_info: Optional[str], shap_info: Optional[str], kde_desc: str, violin_desc: str, radar_desc: str, bipartite_desc: str, output_path: str, model_mistral=None, tokenizer_mistral=None, model_grok=None, tokenizer_grok=None, model_grok_enhanced=None, tokenizer_grok_enhanced=None) -> None:\n",
        "    \"\"\"Generates an insights report using LLMs (using loaded models).\"\"\"\n",
        "\n",
        "    try:\n",
        "        mistral_insights = \"LLM Insights Unavailable (Mistral).\"\n",
        "        grok_insights = \"LLM Insights Unavailable (Grok).\"\n",
        "        grok_enhanced_insights = \"LLM Insights Unavailable (Grok Enhanced).\"\n",
        "\n",
        "        if USE_UNSLOTH_LLM and model_mistral and tokenizer_mistral:\n",
        "            mistral_insights = analyze_text_with_llm(\n",
        "                f\"Analyze: {summary_text}\\n{causal_info}\", MODEL_MISTRAL_NAME, model_mistral, tokenizer_mistral\n",
        "            )\n",
        "        if USE_UNSLOTH_LLM and model_grok and tokenizer_grok:\n",
        "            grok_insights = analyze_text_with_llm(\n",
        "                f\"Analyze: {kde_desc}\\n{violin_desc}\\n{radar_desc}\\n{bipartite_desc}\\n{shap_info}\",\n",
        "                MODEL_GROK_NAME, model_grok, tokenizer_grok\n",
        "            )\n",
        "        if USE_UNSLOTH_LLM and model_grok_enhanced and tokenizer_grok_enhanced:\n",
        "            grok_enhanced_insights = analyze_text_with_llm(\n",
        "                \"Synthesize insights.\", MODEL_GROK_ENHANCED_NAME, model_grok_enhanced, tokenizer_grok_enhanced\n",
        "            )\n",
        "\n",
        "        combined_insights = (\n",
        "            f\"Mistral: {mistral_insights}\\nGrok: {grok_insights}\\nGrok-Enhanced: {grok_enhanced_insights}\"\n",
        "        )\n",
        "\n",
        "        with open(os.path.join(output_path, \"insights.txt\"), \"w\") as f:\n",
        "            f.write(combined_insights)\n",
        "        print(f\"Insights saved to: {os.path.join(output_path, 'insights.txt')}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating insights report: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# --- Main Script ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_output_directory(OUTPUT_PATH)\n",
        "\n",
        "    # Load data\n",
        "    df = load_data_from_string(SYNTHETIC_DATASET)\n",
        "\n",
        "    required_columns = [\n",
        "        PARTICIPANT_ID_COLUMN,\n",
        "        GROUP_COLUMN,\n",
        "        ERRORS_COLUMN,\n",
        "        TASK_DIFFICULTY_COLUMN,\n",
        "        ERROR_TYPE_1_COLUMN,\n",
        "        ERROR_TYPE_2_COLUMN,\n",
        "        ERROR_TYPE_3_COLUMN,\n",
        "        TASK_FEATURE_1_COLUMN,\n",
        "        TASK_FEATURE_2_COLUMN,\n",
        "        TASK_FEATURE_3_COLUMN,\n",
        "    ]\n",
        "    if not validate_dataframe(df, required_columns):\n",
        "        exit(1)  # Exit with an error code\n",
        "\n",
        "    # Data Preprocessing\n",
        "    df, df_original = preprocess_data(df)\n",
        "\n",
        "    # Causal Structure Discovery\n",
        "    causal_variables = [\n",
        "        col for col in df.columns if col.startswith(f\"{GROUP_COLUMN}_\")\n",
        "    ] + [\n",
        "        ERRORS_COLUMN,\n",
        "        TASK_DIFFICULTY_COLUMN,\n",
        "        ERROR_TYPE_1_COLUMN,\n",
        "        ERROR_TYPE_2_COLUMN,\n",
        "        ERROR_TYPE_3_COLUMN,\n",
        "        TASK_FEATURE_1_COLUMN,\n",
        "        TASK_FEATURE_2_COLUMN,\n",
        "        TASK_FEATURE_3_COLUMN,\n",
        "    ]\n",
        "    causal_edges_info = discover_causal_structure(\n",
        "        df.copy(), causal_variables, OUTPUT_PATH\n",
        "    )\n",
        "\n",
        "    # Feature Importance (SHAP)\n",
        "    shap_features = [\n",
        "        col for col in df.columns if col.startswith(f\"{GROUP_COLUMN}_\")\n",
        "    ] + [\n",
        "        TASK_DIFFICULTY_COLUMN,\n",
        "        ERROR_TYPE_1_COLUMN,\n",
        "        ERROR_TYPE_2_COLUMN,\n",
        "        ERROR_TYPE_3_COLUMN,\n",
        "        TASK_FEATURE_1_COLUMN,\n",
        "        TASK_FEATURE_2_COLUMN,\n",
        "        TASK_FEATURE_3_COLUMN,\n",
        "    ]\n",
        "    shap_analysis_info = calculate_shap_values(\n",
        "        df, shap_features, ERRORS_COLUMN, OUTPUT_PATH\n",
        "    )\n",
        "\n",
        "    # Visualization\n",
        "    plt.style.use(\"dark_background\")\n",
        "    neon_colors = [\"#FF00FF\", \"#00FFFF\", \"#FFFF00\", \"#00FF00\"]\n",
        "\n",
        "    create_visualizations(df, df_original, OUTPUT_PATH, neon_colors)\n",
        "\n",
        "    # Bipartite Graph\n",
        "    if shap_analysis_info: # Only create if SHAP values were successful\n",
        "        model_rf = RandomForestRegressor(random_state=42).fit(\n",
        "            df[shap_features], df[ERRORS_COLUMN]\n",
        "        )\n",
        "        explainer = shap.TreeExplainer(model_rf)\n",
        "        shap_values = explainer.shap_values(df[shap_features])\n",
        "        feature_nodes = create_feature_nodes(df, shap_values, shap_features)\n",
        "        visualize_bipartite_graph(df, feature_nodes, OUTPUT_PATH, neon_colors)\n",
        "        print(\"Bipartite graph of participants and features created.\")\n",
        "\n",
        "\n",
        "    # Statistical Analysis\n",
        "    summary_stats_text = perform_statistical_analysis(df, OUTPUT_PATH)\n",
        "\n",
        "\n",
        "    # Load LLMs using Unsloth if GPU is available\n",
        "    model_mistral, tokenizer_mistral, model_grok, tokenizer_grok, model_grok_enhanced, tokenizer_grok_enhanced = None, None, None, None, None, None\n",
        "\n",
        "    if USE_UNSLOTH_LLM:\n",
        "        max_seq_length = 2048  # Choose a suitable sequence length\n",
        "        dtype = None  # Auto-detect dtype\n",
        "        load_in_4bit = True  # Use 4-bit quantization\n",
        "\n",
        "        # Mistral\n",
        "        try:\n",
        "            model_mistral, tokenizer_mistral = FastLanguageModel.from_pretrained(\n",
        "                model_name=MODEL_MISTRAL_NAME,\n",
        "                max_seq_length=max_seq_length,\n",
        "                dtype=dtype,\n",
        "                load_in_4bit=load_in_4bit,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading Mistral model: {e}\")\n",
        "\n",
        "        # Grok (using Mistral as a stand-in, replace if you have a Grok model)\n",
        "        try:\n",
        "            model_grok, tokenizer_grok = FastLanguageModel.from_pretrained(\n",
        "                model_name=MODEL_GROK_NAME,\n",
        "                max_seq_length=max_seq_length,\n",
        "                dtype=dtype,\n",
        "                load_in_4bit=load_in_4bit,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading Grok model: {e}\")\n",
        "\n",
        "        # Grok-Enhanced (using Mistral as a stand-in)\n",
        "        try:\n",
        "            model_grok_enhanced, tokenizer_grok_enhanced = FastLanguageModel.from_pretrained(\n",
        "                model_name=MODEL_GROK_ENHANCED_NAME,\n",
        "                max_seq_length=max_seq_length,\n",
        "                dtype=dtype,\n",
        "                load_in_4bit=load_in_4bit,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading Grok Enhanced model: {e}\")\n",
        "\n",
        "\n",
        "    # Generate Insights Report\n",
        "    generate_insights_report(\n",
        "        summary_stats_text,\n",
        "        causal_edges_info,\n",
        "        shap_analysis_info,\n",
        "        f\"KDE plot of {ERRORS_COLUMN} and {TASK_DIFFICULTY_COLUMN}\",\n",
        "        f\"Violin plot: {ERRORS_COLUMN} by {GROUP_COLUMN}\",\n",
        "        f\"Radar plot: {ERRORS_COLUMN} vs {PARTICIPANT_ID_COLUMN}\",\n",
        "        \"Bipartite graph of participants and features\",\n",
        "        OUTPUT_PATH,\n",
        "        model_mistral, tokenizer_mistral,\n",
        "        model_grok, tokenizer_grok,\n",
        "        model_grok_enhanced, tokenizer_grok_enhanced\n",
        "    )\n",
        "\n",
        "\n",
        "    # DDQN Agent Training\n",
        "    state_dims = [1] * (len(shap_features))  # Each feature is a dimension\n",
        "    action_dim = 2  # Keep it simple for demonstration\n",
        "    ddqn_agent = DDQNAgent(state_dims, action_dim)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        batch = []\n",
        "        for _, row in df.iterrows():\n",
        "            # Create state as a numpy array\n",
        "            state = np.array([row[col] for col in shap_features])\n",
        "            action = ddqn_agent.act(state)\n",
        "            reward = -row[ERRORS_COLUMN]  # Negative of errors\n",
        "            next_state = state # In this simplified example, next_state = state\n",
        "            batch.append((state, action, reward, next_state))\n",
        "        ddqn_agent.learn(batch)\n",
        "        ddqn_agent.update_target_network()  # Update target network each epoch\n",
        "    print(\"DDQN Agent Training Completed\")\n",
        "\n",
        "    # Save Processed Data\n",
        "    df.to_csv(f\"{OUTPUT_PATH}output_data.csv\", index=False)\n",
        "    print(\"Execution completed successfully.\")\n",
        "\n",
        "    # --- Unsloth Fine-tuning and QLoRA (Example) ---\n",
        "    # This section demonstrates *how* you would fine-tune.  It's not a complete\n",
        "    # training loop, but shows the core Unsloth steps.  You would integrate this\n",
        "    # into a proper training setup with your dataset.\n",
        "\n",
        "    if USE_UNSLOTH_LLM and model_mistral and tokenizer_mistral: # Only proceed if LLMs and GPU are available\n",
        "        print(\"\\n--- Fine-tuning section starting ---\")\n",
        "        #  Prepare the model for QLoRA\n",
        "        try:\n",
        "            model_mistral = FastLanguageModel.get_peft_model(\n",
        "                model_mistral,\n",
        "                r=16,  # LoRA rank\n",
        "                target_modules=[\n",
        "                    \"q_proj\",\n",
        "                    \"k_proj\",\n",
        "                    \"v_proj\",\n",
        "                    \"o_proj\",\n",
        "                    \"gate_proj\",\n",
        "                    \"up_proj\",\n",
        "                    \"down_proj\",\n",
        "                ],\n",
        "                lora_alpha=16,\n",
        "                lora_dropout=0,  # Optimized\n",
        "                bias=\"none\",  # Optimized\n",
        "                use_gradient_checkpointing=\"unsloth\",  # Optimized\n",
        "                random_state=3407,\n",
        "                use_rslora=False,\n",
        "            )\n",
        "\n",
        "            #  Create a dummy dataset for demonstration.  Replace with your actual data.\n",
        "            from datasets import Dataset\n",
        "            dataset = Dataset.from_dict({\n",
        "                \"text\": [\n",
        "                    \"Group 1 has high error rates.\",\n",
        "                    \"Task difficulty correlates with errors.\",\n",
        "                    \"Group 2 shows fewer errors.\",\n",
        "                    \"Error type 1 is common in Group 3.\"\n",
        "                ] * 5  # Repeat for more data\n",
        "            })\n",
        "\n",
        "\n",
        "            #  Set up the SFTTrainer (from trl)\n",
        "            from trl import SFTTrainer\n",
        "            from transformers import TrainingArguments\n",
        "\n",
        "            trainer = SFTTrainer(\n",
        "                model=model_mistral,\n",
        "                tokenizer=tokenizer_mistral,\n",
        "                train_dataset=dataset,\n",
        "                dataset_text_field=\"text\",\n",
        "                max_seq_length=max_seq_length,\n",
        "                packing=False,  # Can speed up training for short sequences\n",
        "                args=TrainingArguments(\n",
        "                    per_device_train_batch_size=2,\n",
        "                    gradient_accumulation_steps=4,\n",
        "                    warmup_steps=5,\n",
        "                    max_steps=10,  # Keep it short for demonstration\n",
        "                    learning_rate=2e-4,\n",
        "                    fp16=not torch.cuda.is_bf16_supported(),\n",
        "                    bf16=torch.cuda.is_bf16_supported(),\n",
        "                    logging_steps=1,\n",
        "                    optim=\"adamw_8bit\",\n",
        "                    weight_decay=0.01,\n",
        "                    lr_scheduler_type=\"linear\",\n",
        "                    seed=3407,\n",
        "                    output_dir=OUTPUT_PATH,\n",
        "                    report_to=\"none\",  # Disable reporting for this example\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            #  Train (this is a very short example training run)\n",
        "            trainer.train()\n",
        "\n",
        "            # Save model and tokenizer\n",
        "            trainer.save_model(f\"{OUTPUT_PATH}fine_tuned_model\")\n",
        "            tokenizer_mistral.save_pretrained(f\"{OUTPUT_PATH}fine_tuned_model\")\n",
        "            print(f\"Fine-tuned model saved to {OUTPUT_PATH}fine_tuned_model\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during fine-tuning: {e}\")\n",
        "    else:\n",
        "        print(\"\\n--- Fine-tuning section skipped due to missing GPU or LLM setup ---\")\n",
        "\n",
        "\n",
        "    # --- GGUF Quantization (using llama.cpp) ---\n",
        "    # This part requires llama.cpp to be installed and accessible.\n",
        "    # We'll use a subprocess call for demonstration.  In a real setup,\n",
        "    # you might use the llama.cpp Python bindings.\n",
        "\n",
        "    if USE_UNSLOTH_LLM and model_mistral and tokenizer_mistral: # Only proceed if LLMs and GPU are available\n",
        "        print(\"\\n--- GGUF Quantization section starting ---\")\n",
        "        # 1. Convert the fine-tuned model to HF format (if needed)\n",
        "        #    Unsloth saves in a compatible format, so this step might be skippable.\n",
        "        #    But for completeness, here's how you'd generally do it:\n",
        "        # from transformers import AutoModelForCausalLM\n",
        "        # model = AutoModelForCausalLM.from_pretrained(f\"{OUTPUT_PATH}fine_tuned_model\")\n",
        "        # model.save_pretrained(f\"{OUTPUT_PATH}fine_tuned_hf\")\n",
        "\n",
        "        # 2. Convert to GGUF\n",
        "        import subprocess\n",
        "\n",
        "        # The path to your llama.cpp 'convert' script.  ADJUST THIS!\n",
        "        llama_cpp_convert_path = \"/path/to/your/llama.cpp/convert.py\"  #  **IMPORTANT:  Replace with your actual path!**\n",
        "        # The path to the fine-tuned HF model (adjust if you did the conversion)\n",
        "        hf_model_path = f\"{OUTPUT_PATH}fine_tuned_model\"\n",
        "        # Output GGUF file path\n",
        "        gguf_output_path = f\"{OUTPUT_PATH}fine_tuned_model.gguf\"\n",
        "\n",
        "        # Construct the conversion command.  We're using Q4_0 quantization.\n",
        "        convert_command = [\n",
        "            \"python3\",\n",
        "            llama_cpp_convert_path,\n",
        "            hf_model_path,\n",
        "            \"--outfile\",\n",
        "            gguf_output_path,\n",
        "            \"--outtype\",\n",
        "            \"q4_0\",  # Quantization type\n",
        "        ]\n",
        "\n",
        "        # Run the conversion.  This might take a while!\n",
        "        try:\n",
        "            subprocess.run(convert_command, check=True, capture_output=True, text=True)\n",
        "            print(f\"GGUF model saved to: {gguf_output_path}\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error during GGUF conversion:\\n{e.stderr}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Could not find the llama.cpp convert script at {llama_cpp_convert_path}.  Please install llama.cpp and update the path.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during GGUF conversion: {e}\")\n",
        "    else:\n",
        "        print(\"\\n--- GGUF Quantization section skipped due to missing GPU or LLM setup ---\")"
      ]
    }
  ]
}